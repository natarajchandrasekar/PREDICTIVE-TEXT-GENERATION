# -*- coding: utf-8 -*-
"""Next_word_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-YpGA7XRUFDUu0DyXvaQdJP6jEmPT7yM

**Corpus to train**
"""

#read the corpus(this is a sample file, upload task specific corpus in corpus text file and proceed)
corpus=open("corpus.txt").read()

corpus

#preprocess the corpus
import re
corpus=corpus.lower()
clean_corpus=re.sub('[^a-z0-9]+',' ', corpus)

clean_corpus

""" **Data Preparation**"""

#required libraries
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
import numpy as np

#tokenizing the text into words
tokens = word_tokenize(clean_corpus)
tokens

#length of the sequence to train
train_len = 3

#converting the data into required sequence
text_sequences = []
for i in range(train_len,len(tokens)+1):
  seq = tokens[i-train_len:i]
  text_sequences.append(seq)

text_sequences

#converting the texts into integer sequence
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_sequences)
sequences = tokenizer.texts_to_sequences(text_sequences)
sequences

sequences=np.asarray(sequences)

#vocabulary size
vocabulary_size = len(tokenizer.word_counts)+1
vocabulary_size

#trainX
train_inputs=sequences[:,:-1]

train_inputs

#input sequence length
seq_length=train_inputs.shape[1]
seq_length

#trainY
train_targets=sequences[:,-1]

train_targets

#one hot encoding
train_targets = to_categorical(train_targets, num_classes=vocabulary_size)

train_targets

"""**Let's build the model!**"""

#required libraries
import torch
from torch.optim import Adam
import torch.nn as nn

#lstm model
class lstm(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super().__init__()
        #simple lookup table that stores embeddings of a fixed dictionary and size.
        self.embed = nn.Embedding(vocab_size, embed_size)

        #lstm
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=False)

        #fully connected layer
        self.linear = nn.Linear(hidden_size*seq_length,vocab_size)

    def forward(self, input_word):
        #input sequence to embeddings
        embedded = self.embed(input_word)

        #passing the embedding to lstm model
        output, hidden = self.lstm(embedded)

        #reshaping
        output=output.view(output.size(0), -1)

        #fully connected layer
        output = self.linear(output)
        return output,hidden

model=lstm(vocab_size=vocabulary_size,embed_size=128, hidden_size=256)

model

#Adam optimizer
optimizer= Adam(model.parameters(), lr=0.07)

#loss
criterion = nn.BCEWithLogitsLoss()

#training the model
def train(epoch):
    #set the model to train
    model.train()
    tr_loss=0

    #clearing the Gradients
    optimizer.zero_grad()

    #predict the output
    y_pred, (state_h, state_c) = model(torch.from_numpy(train_inputs))

    #compute the loss
    loss=criterion(y_pred,torch.from_numpy(train_targets))
    losses.append(loss)

    #backpropagate
    loss.backward()

    #update the parameters
    optimizer.step()
    tr_loss = loss.item()

    print("Epoch : ",epoch,"loss : ",loss)

#number of epoch
no_epoch=50
losses=[]
for epoch in range(1,no_epoch+1):
    train(epoch)

!pip install torch

import torch
import matplotlib.pyplot as plt

# Print the type of the losses variable
print(type(losses))

import numpy as np
np.ndarray

print(type(losses))

losses = torch.tensor(losses)

losses = losses.detach()

losses = losses.numpy()

import matplotlib.pyplot as plt
plt.plot(losses)
plt.title('Loss Graph')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# plot the accuracy

import matplotlib.pyplot as plt
import numpy as np
# Convert the losses list to a NumPy array
losses_array = np.array(losses)

# Calculate the number of epochs
num_epochs = len(losses_array)

# Calculate the accuracy for each epoch
accuracy = 1 - losses_array

# Plot the accuracy
plt.plot(range(num_epochs), accuracy)

# Set the title and axis labels
plt.title('Accuracy Graph')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

# Show the plot
plt.show()

#  calculate the accuracy of the model

import numpy as np
# Convert the losses list to a NumPy array
losses_array = np.array(losses)

# Calculate the number of epochs
num_epochs = len(losses_array)

# Calculate the accuracy for each epoch
accuracy = 1 - losses_array

# Calculate the average accuracy
Accuracy = np.mean(accuracy)

# Print the average accuracy
print("Accuracy of the Model :", abs(Accuracy))

"""**Prediction**"""

def predict_next_word(text):
    # Set the model to evaluation mode
    model.eval()

    # Preprocess the input text
    text = text.lower().strip()

    # Tokenize the input text
    input_tokens = word_tokenize(text)

    # Convert tokens to integer sequence
    sequences = tokenizer.texts_to_sequences([input_tokens])

    # Convert sequences to numpy array
    sequences = np.asarray(sequences)

    with torch.no_grad():
        # Convert sequences to tensor and ensure it's of type torch.LongTensor
        sequences = torch.from_numpy(sequences).long()

        # Predict the output
        predict, (hidden, cell) = model(sequences)

    # Get the output from the LSTM
    output = predict[-1]  # Get the output of the last time step

    # Apply softmax layer
    softmax = torch.exp(output)
    prob = list(softmax.numpy())

    # Get the index of the predicted word
    predictions = np.argmax(prob)

    # Convert the index back to word
    next_word = tokenizer.sequences_to_texts([[predictions]])
    return next_word[0]  # Return the first prediction

"""*Example-1*"""

#we trained our model with sequence length of 2
input_text=input("Give Input Text :")
print(input_text)

print("Possible next word will be:")
predict_next_word(input_text)

"""*Example-2*"""

#we trained our model with sequence length of 2
input_text="emails without"

print("Possible next word will be:")
predict_next_word(input_text)